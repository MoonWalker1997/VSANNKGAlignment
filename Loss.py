import torch
import torch.nn.functional as F
from scipy.optimize import linear_sum_assignment

from Utils import cosine_similarity


def knowledge_loss(vec_1, vec_2, space_1, space_2):
    """
    When vec_1 (from space_1) and vec_2 (from space_2) are matched, decompose these vectors to symbols (head, relation,
    tail) and calculate the loss of these symbols.
    """
    H_1, R_1, T_1 = space_1.restore(vec_1)  # get H1, R1, T1
    H_2, R_2, T_2 = space_2.restore(vec_2)  # get H1, R1, T1
    loss_H = F.cosine_embedding_loss(H_1, H_2, torch.ones(H_1.shape[0]))  # use cosine similarity loss
    loss_R = F.cosine_embedding_loss(R_1, R_2, torch.ones(R_1.shape[0]))
    loss_T = F.cosine_embedding_loss(T_1, T_2, torch.ones(T_1.shape[0]))

    return loss_H + loss_R + loss_T


def custom_loss(outputs, targets, learnable_KG, given_KG, learnable_space, given_space, x, x_p, mapping_i, knowledge,
                learnable_S, learn_VSA):
    """
    The comprehensive loss.
    :param outputs: The outputs of the model for the specific task. Here, it is the MNIST classification task.
    :param targets: The label of the specific task.
    :param learnable_KG: The vectors from the knowledge graph generated by the model, K_NN.
    :param given_KG: The vectors of the given knowledge graph, K_G.
    :param learnable_space: VSA_NN.
    :param given_space: VSA_G.
    :param x: The output of the decoder.
    :param x_p: The label of the decoder.
    :param mapping_i: The mapping of concepts in VSA_NN and VSA_G.
    :param knowledge: The knowledge in KG_G, in triplets (as a list, [H, R, T]).
    :param learnable_S: KGV_NN.
    :param learn_VSA: Whether to train VSA_NN.
    """
    # task specific loss
    task_specific_loss = F.nll_loss(outputs, targets)

    KG_matching_loss = 0
    regulation_loss_1 = 0
    regulation_loss_2 = 0
    if learn_VSA:
        # match two sets of vectors
        cos_similarity = cosine_similarity(learnable_KG, given_KG)
        cost = -cos_similarity
        row, col = linear_sum_assignment(cost.detach())

        KG_matching_loss = knowledge_loss(learnable_KG[row], given_KG[col], learnable_space, given_space)

        # regulator 1
        cos_similarity = cosine_similarity(learnable_space.codes_er, learnable_space.codes_er)
        cos_similarity -= torch.diag_embed(cos_similarity.diag())
        regulation_loss_1 = cos_similarity.sum() / (cos_similarity.shape[0] ** 2)

        # regulator 2
        regulation_loss_2 = torch.min((learnable_space.codes_er + 1) ** 2, (learnable_space.codes_er - 1) ** 2).mean()

    # autoencoder loss
    reconstruction_loss = F.mse_loss(x, x_p)

    # use KG_G as the label
    KG_content_loss = 0
    tmp_map = {0: "zero", 1: "one", 2: "two", 3: "three", 4: "four", 5: "five", 6: "six", 7: "seven", 8: "eight",
               9: "nine"}
    if mapping_i is not None:
        S_mask = learnable_S.clone()
        for i in range(S_mask.shape[0]):
            for each in knowledge:
                if each[0] == tmp_map[targets[i].item()]:
                    if each[0] in mapping_i and each[1] in mapping_i and each[2] in mapping_i:
                        S_mask[i, mapping_i[each[1]] - len(learnable_space.entities), mapping_i[each[0]], mapping_i[
                            each[2]]] = 1

        KG_content_loss = F.mse_loss(learnable_S, S_mask)

    return KG_content_loss, (
                task_specific_loss + KG_matching_loss + regulation_loss_1 + regulation_loss_2 + reconstruction_loss + KG_content_loss)
